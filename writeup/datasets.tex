
\section{Datasets}
For this investigation we focus on two corpora: the Wall Street Journal (WSJ) corpus and the Brown corpus. The WSJ corpus is a collection of 2,499 articles collected in a span of three years from the Wall Street Journal newspaper. It has approximately 3 million words and was tagged by using statistically-based methods. This corpus has a total of 82 possible POS tags~\cite{wsjCorpus}. On the other hand, the Brown corpus is a manually tagged collection of 500 text documents sampled from 1961. It uses 36 POS tags and it is consolidated from various sources and from various topics such as fiction, press, and lore~\cite{wsjCorpus}.

These two corpora are chosen because they are standards of the field, making them easy to obtain and allow us to compare our results to the rest of the NLP literature. In addition, they represent different extremes in distributions of words. The range of topics for the WSJ corpus is relatively narrow and specialized, thus it is reasonable to expect that the word choice distribution in the articles to be narrower than other corpora. Using similar logic, we expect the Brown corpus to represent a more general distribution on word choice. Therefore, by using two contrasting corpora are able to gauge the performance of the models in a broader set of situations in which POS tagging is used.

\begin{figure}[ht]
 \begin{Verbatim}[frame=single,framesep=5mm]
\[ He/PRP \]
tried/VBD to/TO ignore/VB
\[ what/WP \]

\[ his/PRP\$ own/JJ common/JJ sense/NN \]
told/VBD
\[ him/PRP \]
,/, but/CC
\[ it/PRP \]

\[ was/VBD n't/RB possible/JJ \]
;/: ;/:
\[ her/PRP$ motives/NNS \]
were/VBD too/RB blatant/JJ ./.
\end{Verbatim}
\caption{Example sentence from the Brown Corpus~\cite{brownCorpus} \label{brownExample}}
\end{figure}
