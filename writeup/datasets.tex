\section{Datasets}
\label{sec:datasets}
For this investigation we focus on two corpora: the Wall Street Journal (WSJ) corpus and the Brown corpus. The WSJ corpus is a collection of 2,499 articles from the Wall Street Journal newspaper span across three years. It has approximately 3 million words and was tagged by using statistically-based methods. This corpus has a total of 82 possible POS tags~\cite{wsjCorpus}. On the other hand, the Brown corpus is a manually tagged collection of 500 text documents sampled from 1961. It uses 93 POS tags and it is consolidated from various sources and contains numerous topics such as fiction, press, and lore~\cite{brownCorpus}.

These two corpora are chosen because they are standards of the field, making them easy to obtain and allow us to compare our results to the rest of the NLP literature. In addition, they represent different distributions of words. Since the WSJ is targeted towards a specific demographics, the range of topics for the WSJ corpus is relatively narrow and should contain more specialized words, thus it is reasonable to expect that the word distribution in the articles to be narrower than other corpora. On the other hand, we expect the Brown corpus to represent a more general distribution on word choice. Therefore, by using two corpora with different properties, we are able to gauge the performance of the models in a broader set of situations in which POS tagging is used.

\begin{figure}[ht]
 \begin{Verbatim}[frame=single,framesep=5mm]
\[ He/PRP \]
tried/VBD to/TO ignore/VB
\[ what/WP \]

\[ his/PRP\$ own/JJ common/JJ sense/NN \]
told/VBD
\[ him/PRP \]
,/, but/CC
\[ it/PRP \]

\[ was/VBD n't/RB possible/JJ \]
;/: ;/:
\[ her/PRP$ motives/NNS \]
were/VBD too/RB blatant/JJ ./.
\end{Verbatim}
\caption{Example sentence from the Brown Corpus~\cite{brownCorpus} \label{brownExample}}
\end{figure}
