% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{csvsimple}
% \linespread{2}
\begin{document}

\numberofauthors{2}
\author{
\alignauthor
Ivan Oropeza\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{University of Texas at Austin}\\
\alignauthor
William Xie\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{University of Texas at Austin}\\
}

\title{Hidden Markov Models vs. Maximum Entropy Markov Models}


\maketitle
\section{Introduction}
[TODO:make it more motiviating]A frequent problem in many disciplines is the challenge to do sequence labelling. DNA sequencing, video semantic analysis, and part-of-speech tagging are just some of the examples where sequence labelling is being used.~\cite{dnaEx, videoEx, nlpEx}. In natural language Processing (NLP), part-of-speech (POS) or lexical category tagging is an important stepping stone to solving more complex problems such as semantic analysis of sentences. The typical techniques applied to this problem are hidden Markov models (HMM), maximum entropy markov models (MEMM), and conditional random fields (CRF)~\cite{nlpBook}. While CRF is considered to be the state-of-the-art for POS tagging, [TODO: why MEMM and HMM and not CRF]. [TODO: kinda redundant for such sort paper;also talka bout implementation]This paper is structured in the following way: first we provide a brief description of the corpora used in this investigation. Then, we discuss some of the differences between HMMs and MEMMs. Next, we explain how the training process works specifically for POS tagging. Finally, we compare the performance of HMMs vs MEMMs under similar circumstances.

\section{Datasets}
For this investigation we focus on two corpora: the Wall Street Journal (WSJ) corpus and the Brown corpus. The WSJ corpus is a collection of 2,499 articles collected in a span of three years from the Wall Street Journal newspaper. It has approximately 3 million words and was tagged by using statistically-based methods. This corpus has a total of 82 possible POS tags~\cite{wsjCorpus}. On the other hand, the Brown corpus is a manually tagged collection of 500 text documents sampled from 1961. It uses 36 POS tags and it is consolidated from various sources and from various topics such as fiction, press, and lore~\cite{brownCorpus}. 

These two corpora are chosen because they are standards of the field, making them easy to obtain and allow us to compare our results with the NLP literature. In addition, they represent different extremes in distributions of words. The range of topics for the WSJ corpus is relatively narrow and thus it is reasonable to expect that the word choice distribution in the articles to be narrower than other corpora. Using similar logic, we expect the Brown corpus to represent a more general distribution on word choice. Therefore, by using two contrasting corpora we will be able to gauge the performance of the models in a broader set of problems in which POS tagging is be used.

\begin{figure}[ht]
 \begin{Verbatim}[frame=single,framesep=5mm]
\[ He/PRP \]
tried/VBD to/TO ignore/VB
\[ what/WP \]

\[ his/PRP\$ own/JJ common/JJ sense/NN \]
told/VBD
\[ him/PRP \]
,/, but/CC
\[ it/PRP \]

\[ was/VBD n't/RB possible/JJ \]
;/: ;/:
\[ her/PRP$ motives/NNS \]
were/VBD too/RB blatant/JJ ./.
\end{Verbatim}
\caption{Example sentence from the Brown Corpus~\cite{brownCorpus} \label{brownExample}}
\end{figure}

\section{HMM vs. MEMM}
The HMM is generative model for the joint distribution of states (POS tags) and observations (words). The state transition follows the Markov assumption. In other words, the transitions between states is restricted to be dependent only on the immediate past~\cite{nlpBook}. On the other hand, MEMM extends the maximum entropy classifier. It is a discriminative model that captures the conditional probability of the current state given the observation and the previous state. Figure \ref{hmmVmemm} shows pictorially the difference between both models.

The MEMM, like the maximum entropy model it is based on, are multinomial logistic regressions to do classification but they focus on making the fewest number of assumptions. While the HMM only uses two aspects of the problem: the transition probabilities for states $P( S_i | S_{i-1} )$ and emission probabilities for current state and observation $P( O_i | S_i )$, MEMM integrates feature information from the observations in addition to the previous state knowledge in order to derive a more accurate prediction model. For example, capitalization is closely associated with proper nouns and specific suffixes, such as "-ed" and "-ing", tend to be associated with verbs. These rules are useful features that can help improve the accuracy of the POS tagger not captured in the HMM model. Moreover, this model can be augmented to include features involving additional past states and past or future observations~\cite{nlpBook}. Finally, the typical algorithms used to find the most probable state sequence for HMM can be modified for MEMM without additional overhead~\cite{memmPaper}.

However, MEMM is susceptible to the "label bias problem". This issue generally arises in two forms. First, if a state, $S_i$, has a high probability of transitioning to $S_j$ with $i \neq j$ (low entropy with the extreme case of probability from $S_i$ to $S_j$ is 1), the observation would be less influential during the decoding process. This occurs when a sequence of observations and the corresponding tags appears abnormally frequently in the training set. The other source of bias is during labelling phase. When an observation occurs infrequently in the training set leads to an inaccurate tag, the tags for subsequent observations are affected by the error as well. \cite{labelBiasProblem}

\begin{figure}[ht]
\centering
\includegraphics[width=80mm]{figures/memm.png}
\caption{Hidden Markov Model(top) and Maximum Entropy Markov Model(bottom)~\cite{nlpBook}. \label{hmmVmemm}}
\end{figure}

\section{Learning}
In POS tagging, training a HMM model in a supervised fashion reduces to a simple counting procedure~\cite{nlpBook}. In order to estimate $P( S_i | S_{i-1} )$ and $P( O_i | S_i )$, we can assume that our annotated corpus is a representative sample of the distribution we want to model, we can then define the distributions as follows:

$P( S_i | S_{i-1} ) \approx \hat{P}( S_i | S_{i-1} ) = C( S_i, S_{i-1} )/C( S_{i-1} )$

$P( O_i | S_i ) \approx \hat{P}( O_i | S_i ) = C( O_i, S_i )/C( S_i )$

where $\hat{P}$ is the probability with respect to the corpus and $C( O_i, S_i ), C( S_i, S_{i-1} )$, and $C( S_i )$ are the appropriate occurrence and co-occurrence of the observations and states.

However there is one additional issue that needs to be addressed before the model can be used for tagging: how should the model respond to words it did not encounter during training? If left unattended, then $P( O_i | S_i ) = 0$. Consequently, Viterbi's algorithm will make incorrect decision during the decoding process. The technique we employ to address this issue is Laplace smoothing (also known as additive smoothing)~\cite{laplaceSmooth}. The general idea of this approach is that every state will always have a small emission probability of producing an unseen word (denoted in our case by "<U>"). And every time the model encounters an unknown word it will use $P( <U> | S_i )$ as the emission probability. The small probability is created by setting $C( <U>, S ) = 1$ and incrementing $C( S )$ by 1 for each state $S$ after calculating the transition probabilities and before calculating the emission probabilities.

XXXXXXXX MEMM Learning TODO XXXXXXXXXXX

\section{Experiment}
All our code is implemented in Python. Also, we enhance an general HMM python module with additive smoothing~\cite{hmmCode}. Moreover, we implement MEMM in its entirety including the learning and decoding process.

XXXXXXXX MEMM BROKEN TODO XXXXXXXXXXX

The following tables show the results for both models when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). In addition to evaluating performance on the testing data we also evaluate the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and stdev for the number of tags correctly labelled. The experiments \label{hmmScores}}
\end{figure}

Given that our implementation of MEMM performs considerably worse than expected (possibly caused by a bug or GIS is converging at a local maxima), we can not directly compare the performance of both models for POS tagging as we would have liked. Ideally, we would compare not only the overall performance of the models but also determine which POS tags had a boost in accuracy and also which ones decreased. From our intuition by adding features that help Noun or Verb tagging we should see an increase in performance for Noun and Verbs in MEMMs.

Never the less, the literature shows that MEMM tend to outperform HMM in POS tagging. Brants reports that their HMM POS tagger reaches 96.46\% accuracy in the WSJ corpus while Denis and Sagot's MEMM tagger correctly tags sentences in WSJ 96.96\% of the time~\cite{memmAhmmResultsACL}. Figure ~\ref{allScores} shows the results for various experiments in POS tagging.

\begin{figure}[ht]
  \begin{tabular}{ l | c | c | c | c | r }
    \bfseries Corpus & \bfseries Model & \bfseries Accuracy & \bfseries Authors

    \csvreader[head to column names]{figures/otherResults.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}% specify your coloumns here
    \end{tabular}
    \caption{Accuracy results for various POS tagging experiments. The empty author cells represent our experiments \label{allScores}}
\end{figure}

\bibliographystyle{abbrv}
\bibliography{references}
\end{document}
