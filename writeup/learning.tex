\section{Learning}
\label{sec:learning}
In POS tagging, training a HMM model in a supervised fashion reduces to a simple counting procedure~\cite{nlpBook}. In order to estimate $P( S_i | S_{i-1} )$ and $P( O_i | S_i )$, we can assume that our annotated corpus is a representative sample of the distribution we want to model, we can then define the distributions as follows:

\vspace{-1em}
\begin{equation}
P( S_i | S_{i-1} ) \approx \hat{P}( S_i | S_{i-1} ) = C( S_i, S_{i-1} )/C( S_{i-1} )
\end{equation}
\vspace{-1em}
\begin{equation}
P( O_i | S_i ) \approx \hat{P}( O_i | S_i ) = C( O_i, S_i )/C( S_i )
\end{equation}
where $\hat{P}$ is the probability with respect to the corpus and $C( O_i, S_i ), C( S_i, S_{i-1} )$, and $C( S_i )$ are the appropriate occurrence and co-occurrence of the observations and states.

However there is one additional issue that needs to be addressed before the model can be used for tagging: how should the model respond to words the model did not encounter during training? If left unattended then $P( O_i | S_i ) = 0$ and consequently, Viterbi's algorithm will make incorrect decisions during the predicting process. The technique we employ to address this issue is Laplace smoothing (also known as additive smoothing)~\cite{laplaceSmooth}. The general idea of this approach is that every state will always have a small emission probability of producing an unseen word (denoted in our case by ``$\langle U \rangle $''). And every time the model encounters an unknown word it will use $P(\langle U \rangle| S )$ as the emission probability. The small probability is created by setting $C(\langle U \rangle, S ) = 1$ and incrementing $C( S )$ by 1 for each state $S$ after calculating the transition probabilities and before calculating the emission probabilities.

Training the MEMM is different from the straight forward counting method used in HMM. Since state transition depends on both previous state and the current observation, one large transition probability matrix (TPM) is employed. The TPM encapsulates all combinations of previous states $S_{i-1}$ and current observation $O_i$ pair in the training data to the current state $S_i$. Let $N$ represents the number of unique states and $M$ the number of unique words, the TPM has the shape $N * M \times N$. Each feature $f_a$ is an indicator function that takes $(O_i, S_{i-1})$ as arguments with additional normalizing feature $f_x(O_i, S_i) = C - \sum\limits_{a} f_a(O_i, S_i)$ to ensure $(O, S)$ pairs not captured by specific features don't get penalized. The transitional probability takes the exponential family form:
\vspace{-1em}
\begin{equation}
P_{s_{i-1}}(S_i | O_i) = \frac{1}{Z(O_i, S_{i-1})} exp(\sum\limits_{a}\lambda_a f_a(O_i, S_i))
\end{equation}
where Z is the normalizing factor that makes the TPM sum to 1 across each row and $\lambda_a$ is the weight parameter to be learned.

The training data is divided into $N$ buckets. For each sentence and for each $(word, tag)$ pair in it, the $(O_i, S_i)$ is placed the bucket corresponding to $S_{i-1}$ tag. In other words, the count each word in the sentence is only associated with the previous word's tag instead of globally like that of HMM.  Let $m_{s_{i-1}}$ represent the number of observations per bucket. We then used generalized iterative scaling (GIS) to optimize each $\lambda_a$ so that it satisfies the following property for each tag:

\vspace{-3em}
\begin{equation}
\begin{dmath}
\frac{1}{m_{s_{i-1}}}\sum\limits_{k=1}^{m_{s_{i-1}}} f_a(O_k, S_k) = \\ \frac{1}{m_{s_{i-1}}}\sum\limits_{k=1}^{m_{s_{i-1}}} \sum\limits_{S}P_{s_{i-1}}(S|O_k)f_a(O_k, S)
\end{dmath}
\end{equation}
\vspace{-5em}

where the left hand side is the average occurrence of the feature and the right hand side the expected value of the feature. By continuously updating the $\lambda_a$s using the the log of the ratio between the average and the expected value and TPM using the new $lambda_a$s, the model is fully trained when all $\lambda_a$s converge. The procedure is described in more detail in \cite{memmPaper}.
