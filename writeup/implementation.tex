\section{Implementation and Experiments}
\label{sec:implementation}
We have implemented both HMM and MEMM in Python with NumPy. We leverage on the Viterbi algorithm provided by Hamilton\cite{hmmCode} for HMM. We implemented MEMM entirely from scratch. The MEMM implementation is largely based on McCallum et al.'s algorithm for information extraction\cite{memmPaper}.

MEMM was implemented based on the learning algorithm discribed in section ~\ref{sec:comparison}. The decoding algorithm we used is modified viterbi algorithm. Unknow words are handled using uniform distribution similar to that of HMM. The condition for convergence is when $\lambda_a$ is within 0.1 of the previous value. The precision could be further improved at at the costly expense of computation time. Training with 1000 sentences from WSJ and Brown, the sentence and tag accuracies are (0, 16.72) and (0, 20.67) percents respectively. The following table shows the results when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). We used total of 200 sentences, each portion 10 sentences long. In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

%The following tables show the results for both models when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

% MEMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/memmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{}
\end{figure}

The results we obtained from MEMM is significnatly lower than expected. With the low tag accuracy, 0 sentence accuracy is a reasonable result. We believe this is the result of insufficent training data. The largest training set we were able to run was 1000 sentences. For WSJ and Brown, it generated TPM with sizes 389501 and 49 and 230112 by 48 respectively, comparing to 59670 by 39 and 38136 by 42 for 100 sentences. The computation time increases with the size of TPM making learning from large training data infeasible for the scope of this project. Because MEMM requires dividing observations into buckets, the problem amplifies. For example, if observation ``The'' with tag ``DT'' appears in the data $c$ times, these occurances would be divided among the previous state buckets for ``NN'', ``VB'', ``DT'', etc. Each bucket would have far fewer than the total $c$ occurances, drawing inconclusive probabilities. In addition, with non-trivial chance of error due to small training set, the modifield Viterbi algorithm magnifies the problem by propogating the error through the sentence.

Another reason for the cause of the low accuracy is the possibility of GIS converging to local minima.

HMM, due to simpler model and fasater training in comparison, was able to train using the entire data set. The below is the same 20-fold cross-validation method described above for WSJ and Brown corpora:
% HMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and stdev for the number of tags correctly labelled. The experiments \label{hmmScores}}
\end{figure}

The results shows HMM performs significantly better at Brown in sentence accuracy and than WSJ and marginally better in tag accuracy. [reasons: longer sentences, number of unknown words, or just 'luck' for tags due to randomness]

Given that our implementation of MEMM performs considerably worse than expected, we can not directly compare the performance of both models for POS tagging as we intended. Ideally, we would compare not only the overall performance of the models but also determine which POS tags had a boost in accuracy and also which ones decreased. From our intuition by adding features that help Noun or Verb tagging we should see an increase in performance for Noun and Verbs in MEMMs.

Never the less, the literature shows that MEMM tend to outperform HMM in POS tagging. Brants reports that their HMM POS tagger reaches 96.46\% accuracy in the WSJ corpus while Denis and Sagot's MEMM tagger correctly tags sentences in WSJ 96.96\% of the time~\cite{memmAhmmResultsACL}. Figure ~\ref{allScores} shows the results for various experiments in POS tagging.

\begin{figure}[ht]
  \begin{tabular}{ l | c | c | c | c | r }
    \bfseries Corpus & \bfseries Model & \bfseries Accuracy & \bfseries Authors

    \csvreader[head to column names]{figures/otherResults.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}% specify your coloumns here
    \end{tabular}
    \caption{Accuracy results for various POS tagging experiments. The empty author cells represent our experiments \label{allScores}}
\end{figure}