\section{Implementation and Experiments}
\label{sec:implementation}
We implemented both HMM and MEMM in Python using NumPy. For the HMM model, we leverage on the Viterbi algorithm provided by Hamilton~\cite{hmmCode} and the training was done as per section \ref{sec:learning}. 

On the other hand, we implemented MEMM entirely from scratch. The MEMM implementation is largely based on McCallum et al.'s algorithm for information extraction\cite{memmPaper}. The training for MEMM is implemented based on the learning algorithm described in section ~\ref{sec:learning}. The decoding algorithm we used is modified Viterbi algorithm. Unknown words are handled using uniform distribution in a similar fashion to that of HMM. The condition for convergence is when $\lambda_a$ is within 0.1 of the previous value. The precision could be further improved at at the expense of computation time. Training with 1000 sentences from WSJ and Brown, the sentence and tag accuracies are (0\%, 16.72\%) and (\%0, 20.67\%) respectively. The following table shows the results when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). We used total of 200 sentences, each fold being 10 sentences long. In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

%The following tables show the results for both models when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

% MEMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/memmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{}
\end{figure}

The results we obtained from MEMM are significantly lower than expected. With such low tag accuracy, 0 \% sentence accuracy is a reasonable result. We believe this is the result of insufficient training data. The largest training set we were able to run was 1000 sentences. For WSJ and Brown, it generated TPM with sizes $389501 x 49$ and $230112 x 48$ respectively, compared to the TPMs generated for 100 sentences $59670 x 39$ and $38136 x 42$. The computation time increases with the size of TPM making learning from large training data infeasible for the scope of this project given our GIS implementation. Moreover because learning requires dividing observations into buckets, the problem amplifies. For example, if observation ``The'' with tag ``DT'' appears in the data $c$ times, these occurrences would be divided among the previous state buckets for ``NN'', ``VB'', ``DT'', etc. Each bucket would have far fewer than the total $c$ occurrences, drawing inconclusive probabilities. In addition, with non-trivial chance of error due to small training set, the modified Viterbi algorithm magnifies the problem by propagating the error through the sentence. The final reason for the cause of the low accuracy is the possibility of GIS converging to local minima.

HMM, due to simpler model and faster training in comparison, was able to train using the entire data set. The figure below is the same 20-fold cross-validation experiment but for HMM:
% HMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and stdev for the number of tags correctly labelled. \label{hmmScores}}
\end{figure}

The results show the HMM parser performs significantly better at Brown in sentence accuracy than WSJ and marginally better in tag accuracy. Due to the simple metric we use, the length of the sentence is a crucial factor. On average the Brown corpus has 16 observations per sentence. Meanwhile, WSJ has sentences with 53 observations. Another, factor that may influence this score and the tag accuracy as well is the presence of unknown words. However, due to the fact that on average Brown and WSJ encounter 12.1\% and 11.8\% respectively there is no strong evidence of it. Finally, since the experiments were on the entire dataset, there may be a size disadvantage. Figure \ref{hmmScores1000} shows the results for the same experiment but limiting the number of sentences to 1000. Once again we see that Brown gives better performance.

\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores@1000.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora using only 1000 sentences. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and stdev for the number of tags correctly labelled. \label{hmmScores1000}}
\end{figure}

Given that our implementation of MEMM performs considerably worse than expected, we can not directly compare the performance of both models for POS tagging as we intended. Ideally, we would compare not only the overall performance of the models but also determine which POS tags had a boost in accuracy and also which ones decreased. From our intuition by adding features that help noun or verb tagging we should see an increase in performance for them in MEMMs.

Never the less, the literature shows that MEMM tend to outperform HMM in POS tagging. Brants reports that their HMM POS tagger reaches 96.46\% accuracy in the WSJ corpus while Denis and Sagot's MEMM tagger correctly tags sentences in WSJ 96.96\% of the time~\cite{memmAhmmResultsACL}. Figure ~\ref{allScores} shows the results for various experiments in POS tagging.

\begin{figure}[ht]
  \begin{tabular}{ l | c | c | c | c | r }
    \bfseries Corpus & \bfseries Model & \bfseries Accuracy & \bfseries Authors

    \csvreader[head to column names]{figures/otherResults.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}% specify your coloumns here
    \end{tabular}
    \caption{Accuracy results for various POS tagging experiments. The empty author cells represent our experiments \label{allScores}}
\end{figure}