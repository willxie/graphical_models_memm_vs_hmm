\section{Implementation and Experiments}
\label{sec:implementation}
We implemented both HMM and MEMM in Python using NumPy. For the HMM model, we leverage on the Viterbi algorithm provided by Hamilton~\cite{hmmCode} and the training was done as per section~\ref{sec:learning}. The following table shows the results when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1) on the HMM. The HMM, due to its fast training, was trained using the entire Brown and WSJ data set individually. In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.
% HMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and stdev for the number of tags correctly labled. \label{hmmScores}}
\end{figure}

The results show the HMM parser performs significantly better at Brown in sentence accuracy than WSJ and marginally better in tag accuracy in contrast to our original hypothesis. This may be due to the sentence accuracy metric we use which makes the length of the sentence a crucial factor. On average the Brown corpus has 16 observations per sentence. Meanwhile, WSJ has sentences with 53 observations. Another factor that may influence this score and the tag accuracy as well is the presence of unknown words. However, due to the fact that on average Brown and WSJ encounter unknown words 12.1\% and 11.8\% respectively there is no strong evidence of it. Finally, since the experiments were on the entire dataset, there may be a size disadvantage. Figure \ref{hmmScores1000} shows the results for the same experiment but limiting the number of sentences to 1000. Once again we see that Brown gives better performance.

%Another hypothesis that could explain these results is that because WSJ is targeted toward a demographic with high education background, its vocabulary might be broader. This makes the prediction more error prone when compared to a smaller vocabulary with higher concentration of common words.

\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores@1000.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora using only 1000 sentences.}
\end{figure}



On the other hand, we implemented MEMM entirely from scratch. The MEMM implementation is largely based on McCallum et al.'s algorithm for information extraction~\cite{memmPaper}. The training for MEMM is implemented based on the learning algorithm described in section~\ref{sec:learning}. The predicting algorithm we used is the modification on the Viterbi algorithm described in~\cite{memmPaper}. Unknown words are handled using a uniform distribution in a similar fashion to Laplace smoothing for HMM. The condition for convergence is when the current value of $\lambda_a$ is within 0.1 of the previous value . The precision may be further improved at at the expense of computation time. Training with 1000 sentences from WSJ and Brown, the sentence and tag accuracies are (0\%, 16.72\%) and (0\%, 20.67\%) respectively. The following table shows the results when we execute the same 20-fold experiment except we used total of 200 sentences, with each fold being 10 sentences long.

%The following tables show the results for both models when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

% MEMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/memmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{}
\end{figure}

The results we obtained from MEMM are significantly lower than expected. With such low tag accuracy, 0\% sentence accuracy is a reasonable result. We believe this is the result of insufficient training data. The largest training set we were able to run was 1000 sentences. For WSJ and Brown, it generated TPM with dimensions $389501 \times 49$ and $230112 \times,48$ respectively, compared to the TPMs generated for 100 sentences $59670 \times 39$ and $38136 \times 42$. The computation time increases with the size of TPM making learning from large training data infeasible for the scope of this project given our GIS implementation. The 20-fold epxeriment took around 10 hours for each of the corpora. Moreover, because learning requires dividing observations into buckets, the problem amplifies. For example, if observation ``The'' with tag ``DT'' appears in the training data $c$ times, these occurrences would be divided among the previous state buckets for ``NN'', ``VB'', ``DT'', etc. Each bucket would have far fewer than the total $c$ occurrences, drawing inconclusive probabilities. Following this logic, MEMM benefits more in having a large training sets than HMM. In addition, with the non-trivial chance of error due to small training set, the modified Viterbi's algorithm magnifies the problem by propagating the error through the sentence. Test set performed even worse due to the introduction of smoothing in addition to the problems described above. Another reason for the cause of the low accuracy is the possibility of GIS converging to local maxima. We believe with more training data, the MEMM model could perform significantly better than it is currently.

Given the poor results from our implementation of MEMM, we have tested each part of the program individually and intensively by manually creating ideal data, transitional probabilities, and other features of the model. Through inspection, each part seems to be behave as expected and no unexpected bugs found. When the accuracy of the experiments is low, it is still in line with the performance of HMM with Brown having higher tag accuracy than WSJ. This leads us to believe the main culprit remains to be the small training set.

Given that our implementation of MEMM performs considerably worse than expected due to the insufficent training data, we cannot draw meaningful conclusions based on the results. Ideally, we would compare not only the overall performance of the models but also determine which POS tags have advantages in accuracy for each model. Following our intuition, by adding features that help noun or verb tagging we should see an increase in performance for these tags in MEMMs.

Nevertheless, the literature shows that MEMM tend to outperform HMM in POS tagging. Brants reports that their HMM POS tagger reaches 96.46\% accuracy in the WSJ corpus while Denis and Sagot's MEMM tagger correctly tags sentences in WSJ 96.96\% of the time~\cite{memmAhmmResultsACL}. Figure~\ref{allScores} shows the results for various experiments in POS tagging.

\begin{figure}[ht]
  \begin{tabular}{ l | c | c | c | c | r }
    \bfseries Corpus & \bfseries Model & \bfseries Accuracy & \bfseries Authors

    \csvreader[head to column names]{figures/otherResults.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}% specify your coloumns here
    \end{tabular}
    \caption{Accuracy results for various POS tagging experiments. The empty author cells represent our experiments \label{allScores}}
\end{figure}