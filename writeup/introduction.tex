\section{Introduction}
\label{sec:introduction}

Sequence labeling is a frequently done task across many disciplines. DNA sequencing, video semantic analysis, and part-of-speech tagging are just some of the examples where sequence labeling is being used.~\cite{dnaEx, videoEx, nlpEx}. The tagging task was traditionally done manually by inspection and proved to be a very time consuming and costly task. In natural language processing (NLP), part-of-speech (POS) or lexical category tagging is an important stepping-stone to solving more complex problems such as semantic analysis of sentences. The typical techniques applied to this problem are Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM), and Conditional Random Fields (CRF)~\cite{nlpBook}. We want to explore HMM and MEMM, two directed graphical models that shares the Markov assumption, through implementation and compare and contrast the learning and predicting mechanisms behind them as well as their performances. This paper is structured in the following way: first we provide a brief description of the corpora used in this investigation. Then, we discuss some of the similarities and differences between HMMs and MEMMs. Next, we explain how the training process works specifically for POS tagging. Then, we describe our implementation and experimental setup and analysis the performance of HMMs vs MEMMs under similar circumstances.
