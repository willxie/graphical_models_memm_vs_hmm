\section{Implementation and Results}
\label{sec:implementation}
We implemented both HMM and MEMM in Python using NumPy. For the HMM model, we leverage on the Viterbi algorithm provided by Hamilton~\cite{hmmCode} and the training was done as per section~\ref{sec:learning}. The following table shows the results for HMM when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). The HMM, due to its fast training, was trained using the entire corpus. In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with train data will give us an idea of over fitting (how much generalization is the model doing for this task) and testing with test data will give us an idea of the effectiveness of this model.
% HMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora. The first column is the corpus used. The second column is the data used for evaluating the system. Four scores are reported: the average and stdev for the number of sentences correctly tagged and the average and standard deviation for the number of tags correctly labled. \label{hmmScores}}
\end{figure}

The results show that the HMM parser performs significantly better with Brown in sentence accuracy than WSJ. Additionally, it also performs marginally better in tag accuracy. This contradicts our original hypothesis. For the sentence accuracy, this may be due to the length of the sentences. On average the Brown corpus has 16 observations per sentence. Meanwhile, WSJ has sentences averaging 53 observations. Another factor that may influence this score and the tag accuracy is the presence of unknown words. However, after further investigation, this hypothesis proved to be incorrect. On average Brown and WSJ encounter unknown words 12.1\% and 11.8\%. The data contradicts our hypothesis but the difference is marginal making the effect is inconclusive. Figure \ref{hmmScores1000} shows the results for the same experiment but limiting the number of sentences to 1000. Once again we see that Brown gives better performance (Figure ~\ref{fig:hmm1000}). We also attempted to shuffle the sentences before running the experiment in order mitigate problems related to domain adaptation, giving a more general result. As expected, the Brown corpus yields 93.7\% and 95.8\% for test and train data respectively and WSJ 91.5\% and 93.5\%.

%Another hypothesis that could explain these results is that because WSJ is targeted toward a demographic with high education background, its vocabulary might be broader. This makes the prediction more error prone when compared to a smaller vocabulary with higher concentration of common words.

\begin{figure}[ht]

  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/hmmScores@1000.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{HMM percent scores (\%) for the Brown and WSJ corpora using only 1000 sentences.}
  \label{fig:hmm1000}
\end{figure}


On the other hand, we implemented MEMM entirely from scratch. The MEMM implementation is largely based on McCallum et al.'s algorithm for information extraction~\cite{memmPaper}. The training for MEMM is implemented based on the learning algorithm described in section~\ref{sec:learning}. The decoding algorithm we used is the modification on the Viterbi's algorithm described in~\cite{memmPaper}. Unknown words are handled using a uniform distribution in a similar fashion to Laplace smoothing for HMM. The condition for convergence is when the current value of $\lambda_a$ is within 0.1 of the previous value . The precision may be further improved at the expense of computation time. Training with 1000 sentences from WSJ and Brown corpora, we achieved the sentence and tag accuracies are (0\%, 16.72\%) and (0\%, 20.67\%) respectively. The following table shows the results when we execute the same 20-fold experiment except we used total of 200 sentences.

%The following tables show the results for both models when we execute our experiments using 20-fold cross-validation (train with 19 portions and test on 1). In addition to evaluating performance on the testing data we also evaluated the model's performance on the training data. Testing the model with training data will give us an idea of over fitting (how much generalizable is the model for this task) and testing with testing data will give us an idea of effectiveness of the system.

% MEMM
\begin{figure}[ht]
  \begin{tabular}{ l || c | c | c | c | c }
    \bfseries & \bfseries & \bfseries \overline{Sentence} & \bfseries \sigma Sentence & \bfseries \overline{Tag} & \bfseries \sigma Tag

    \csvreader[head to column names]{figures/memmScores.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi}% specify your coloumns here
    \end{tabular}
    \caption{MEMM percent scores (\%) for the Brown and WSJ corpora using 200 sentences.}
\end{figure}

The results we obtained for MEMM are significantly lower than expected. With such low tag accuracy, 0\% sentence accuracy is a reasonable result. We believe this is the result of insufficient training data. The largest training set we were able to run was 1000 sentences. For WSJ and Brown, it generated TPM with dimensions $389,501 \times 49$ and $230,112 \times 48$ respectively, compared to the TPMs generated for 100 sentences $59,670 \times 39$ and $38,136 \times 42$. The computation time increases with the size of TPM making learning from large training data infeasible for the scope of this project given our GIS implementation. The 20-fold experiment took around 10 hours for each of the corpora. Moreover, because learning requires dividing observations into buckets, the problem amplifies. For example, if the observation ``THE'' with tag ``DT'' appears in the training data $c$ times, the occurrences would be divided among the previous state buckets for ``NN'', ``VB'', ``DT'', etc. Each bucket would have far fewer than the total $c$ occurrences, drawing inconclusive probabilities. Following this logic, MEMM benefits more in having a large training set than HMM. In addition, with the non-trivial chance of error due to small training set, the modified Viterbi's algorithm magnifies the problem by propagating the error through the sentence. Test set performed even worse due to the introduction of smoothing in addition to the aforementioned problems. Another reason for the cause of the low accuracy is the possibility of GIS converging to local maxima. We believe with more training data, the MEMM model could perform significantly better than it is currently performing.

Given the poor results from our implementation of MEMM, we have tested each part of the program individually and intensively by manually creating ideal data, transitional probabilities, and other features of the model. Through inspection, each part seems to be behave as expected and no bugs was found. Even though the accuracy of the experiments is low, it is still in line with the performance of HMM with Brown having higher tag accuracy than WSJ. This leads us to believe the main culprit remains to be the small training set.

Given that our implementation of MEMM performs considerably worse than expected due to the insufficient training data, we cannot draw meaningful conclusions based on the results. Ideally, we would compare not only the overall performance of the models but also determine which POS tags experience an increase in accuracy.

Nevertheless, the literature shows that MEMM tends to outperform HMM in POS tagging. Brants reports that their HMM POS tagger reaches 96.46\% accuracy in the WSJ corpus while Denis and Sagot's MEMM tagger correctly tags 96.96\% for the same corpus. ~\cite{memmAhmmResultsACL}. Figure~\ref{allScores} shows the results for various experiments in POS tagging.

\begin{figure}[ht]
  \begin{tabular}{ l | c | c | c | c | r }
    \bfseries Corpus & \bfseries Model & \bfseries Accuracy & \bfseries Authors

    \csvreader[head to column names]{figures/otherResults.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}% specify your coloumns here
    \end{tabular}
    \caption{Accuracy results for various POS tagging experiments. \label{allScores}}
\end{figure}
